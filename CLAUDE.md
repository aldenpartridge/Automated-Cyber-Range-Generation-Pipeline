# CLAUDE.md - AI Assistant Guide

## Repository Overview

This repository contains the **complete specification** for an automated cyber range generation pipeline designed for university-level CTF (Capture-the-Flag) competitions. The system uses a multi-agent AI architecture to generate fully self-hosted, containerized King-of-the-Hill (KOTH) CTF environments.

**Current State:** Specification/documentation phase - contains detailed architecture but minimal implementation code.

**Primary Document:** `README.md` - comprehensive specification written for downstream LLM agents.

---

## üéØ Core Purpose & Mission

### What This System Does

- Automatically generates complete CTF scenarios from user requirements
- Creates fictional organizations with realistic employee data and credentials
- Builds Docker-based network topologies with 25-30 containers
- Implements King-of-the-Hill scoring mechanisms
- Provides centralized logging (ELK/OpenSearch/Loki stacks)
- Ensures safe, offline-only, high-level vulnerability challenges
- Supports optional simulated mobile devices for MFA challenges

### Critical Constraints

‚úÖ **MUST**:
- Everything fictional (no real people, organizations, or credentials)
- Offline-only execution (no internet connectivity required)
- High-level vulnerabilities only (weak passwords, reuse, misconfigs)
- Resource-efficient (~25-30 containers max for student hardware)
- Full logging of all attacker actions
- Total scenario value: 15,000 points (employee bounty system)

‚ùå **MUST NOT**:
- Use real-world exploit techniques
- Include unsafe code or actual vulnerabilities
- Reference real individuals or organizations
- Require heavy emulators or excessive resources
- Enable any external network access

---

## üîÑ Static Infrastructure vs. Dynamic Scenario Content

### Critical Architectural Principle

The system has **two distinct layers** that must remain separate:

#### üõ†Ô∏è **STATIC INFRASTRUCTURE** (Reusable Tooling - Implement Once)

These components are **consistent across all CTF scenarios** and should be implemented as reusable software:

**Core Software Components:**
- ‚úÖ **Control Beacon Software** - Generic agent that runs on any host (parameterized with tokens)
- ‚úÖ **Scoring Engine** - KOTH logic, token validation, lockout mechanics, points calculation
- ‚úÖ **Scoreboard UI** - Web interface for displaying team scores
- ‚úÖ **Org-Chart Viewer** - Web interface for exploring the fictional organization
- ‚úÖ **Admin Dashboard** - Control panel for monitoring and managing the range
- ‚úÖ **Logging Infrastructure** - ELK/OpenSearch/Loki stack setup and configuration
- ‚úÖ **Deployment Coordinator Scripts** - Validation, health checks, orchestration
- ‚úÖ **Safety Validator** - Automated checks for ethical compliance
- ‚úÖ **Cleanup Scripts** - Teardown automation
- ‚úÖ **Data Seeding Tools** - Scripts that populate containers with generated data

**Base Docker Images:**
- ‚úÖ Base workstation image (Ubuntu/Debian with common tools)
- ‚úÖ Base server image (web server, database, etc.)
- ‚úÖ Base mobile device image (lightweight container simulating a phone)
- ‚úÖ Logging agent image (Filebeat/Promtail/etc.)

**Libraries & Utilities:**
- ‚úÖ Credential generation libraries
- ‚úÖ Fictional data generators (wrapper around Faker, etc.)
- ‚úÖ Network topology validators
- ‚úÖ Docker Compose template generators

#### üé≤ **DYNAMIC SCENARIO CONTENT** (Generated Per CTF)

These artifacts are **unique for each CTF event** and generated by AI agents:

**Scenario Definitions:**
- üé≤ `ORGANIZATION.md` - Unique fictional company, structure, backstory
- üé≤ `EMPLOYEE_CREDENTIALS.json` - Unique employees with custom credentials
- üé≤ `NETWORK_TOPOLOGY.md` - Custom network design for this scenario
- üé≤ `CHALLENGE_DESIGN.md` - Specific attack paths and vulnerabilities
- üé≤ `SEEDING_PLAN.md` - What data goes where for this scenario

**Generated Content:**
- üé≤ Fake emails (content, not email service software)
- üé≤ Documents and PDFs with scenario-specific clues
- üé≤ Log files with scenario-specific hints
- üé≤ Configuration files with intentional misconfigurations
- üé≤ Intranet content for the fictional organization
- üé≤ Control tokens (random, unique per scenario)
- üé≤ Bounty point distribution across employees/hosts

**Instance-Specific Configurations:**
- üé≤ `docker-compose.yml` - Specific to this scenario's topology
- üé≤ User account configurations for each container
- üé≤ Service-specific configs (web server content, database schemas)
- üé≤ Network subnet assignments

### Implementation Guidance

**For Static Infrastructure:**
- Write **generic, parameterized code**
- Use configuration files and environment variables
- Design for reusability across scenarios
- Version control in main codebase
- Thorough testing and documentation
- Examples: `scoring-engine/app.py`, `control-beacon/beacon.py`, `org-viewer/index.html`

**For Dynamic Content:**
- AI agents **consume templates** from static infrastructure
- AI agents **generate data** that populates those templates
- Store generated artifacts in `scenario/v{timestamp}/` directories
- Each scenario is reproducible from seed inputs
- Examples: Specific employee names, passwords, email content, org structure

### Directory Structure with Static/Dynamic Distinction

```
/
‚îú‚îÄ‚îÄ README.md                          # Specification
‚îú‚îÄ‚îÄ CLAUDE.md                          # This file
‚îÇ
‚îú‚îÄ‚îÄ infrastructure/                    # ‚úÖ STATIC - Reusable tooling
‚îÇ   ‚îú‚îÄ‚îÄ control_beacon/               # Generic beacon software
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ beacon.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ scoring_engine/               # KOTH scoring logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scoreboard.html
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ org_viewer/                   # Org chart visualization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ viewer.js
‚îÇ   ‚îú‚îÄ‚îÄ admin_dashboard/              # Admin control panel
‚îÇ   ‚îú‚îÄ‚îÄ logging/                      # Logging stack configs
‚îÇ   ‚îú‚îÄ‚îÄ base_images/                  # Base Dockerfiles
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ workstation/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ server/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mobile/
‚îÇ   ‚îú‚îÄ‚îÄ deployment/                   # Deployment scripts
‚îÇ   ‚îî‚îÄ‚îÄ tools/                        # Seeding/validation tools
‚îÇ
‚îú‚îÄ‚îÄ agents/                            # ‚úÖ STATIC - AI agent implementations
‚îÇ   ‚îú‚îÄ‚îÄ 02_organization_builder/
‚îÇ   ‚îú‚îÄ‚îÄ 03_network_architect/
‚îÇ   ‚îú‚îÄ‚îÄ 04_vulnerability_designer/
‚îÇ   ‚îú‚îÄ‚îÄ 05_image_builder/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ scenarios/                         # üé≤ DYNAMIC - Generated per CTF
    ‚îú‚îÄ‚îÄ scenario_20240115_143022/     # Scenario instance
    ‚îÇ   ‚îú‚îÄ‚îÄ ORGANIZATION.md
    ‚îÇ   ‚îú‚îÄ‚îÄ EMPLOYEE_CREDENTIALS.json
    ‚îÇ   ‚îú‚îÄ‚îÄ NETWORK_TOPOLOGY.md
    ‚îÇ   ‚îú‚îÄ‚îÄ CHALLENGE_DESIGN.md
    ‚îÇ   ‚îú‚îÄ‚îÄ seed_data/                # Generated content
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ emails/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ documents/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logs/
    ‚îÇ   ‚îú‚îÄ‚îÄ build/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml    # Instance-specific
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ service_configs/
    ‚îÇ   ‚îî‚îÄ‚îÄ runtime/
    ‚îÇ       ‚îî‚îÄ‚îÄ logs/
    ‚îî‚îÄ‚îÄ scenario_20240122_091544/     # Different CTF instance
        ‚îî‚îÄ‚îÄ ...
```

### Agent Responsibilities Clarified

| Agent | Static Output | Dynamic Output |
|-------|---------------|----------------|
| **Organization Builder** | None (uses generic templates) | `ORGANIZATION.md`, `EMPLOYEE_CREDENTIALS.json` |
| **Network Architect** | None (uses topology templates) | `NETWORK_TOPOLOGY.md` |
| **Vulnerability Designer** | None | `CHALLENGE_DESIGN.md` |
| **Image Builder** | Base Dockerfiles (if not exist) | Instance `docker-compose.yml`, service configs |
| **Data Seeder** | Seeding scripts (if not exist) | Actual seed data content |
| **Scoring Engine** | Scoring software (first time only) | Scoring configuration for this scenario |
| **Logging Subsystem** | Logging stack (first time only) | Logging filters/rules for this scenario |
| **Admin Dashboard** | Dashboard software (first time only) | Dashboard configuration |

### Key Principle

> **AI agents should CONSUME static tooling and PRODUCE scenario-specific data.**
>
> Think of it like a game engine (static) vs. game levels (dynamic). The engine is written once; levels are generated infinitely.

---

## üèóÔ∏è Multi-Agent Architecture

The pipeline consists of **13 specialized AI agents** that work sequentially. Each agent produces artifacts consumed by downstream agents.

### Pipeline Stages

| # | Agent | Primary Outputs | Key Responsibilities |
|---|-------|----------------|---------------------|
| 1 | **User Input** | Requirements doc | Scenario theme, industry, scale, difficulty |
| 2 | **Organization Builder** | `ORGANIZATION.md`<br>`EMPLOYEE_CREDENTIALS.json` | Fictional org structure, employee profiles, credentials with patterns |
| 3 | **Network Architect** | `NETWORK_TOPOLOGY.md` | Network topology, DMZ/LAN zones, Docker layout, mobile devices |
| 4 | **Vulnerability Designer** | `CHALLENGE_DESIGN.md` | Safe vulnerabilities, attack paths, credential challenges, MFA flows |
| 5 | **Image Builder** | `Dockerfiles/`<br>`docker-compose.yml` | Container definitions, user accounts, control beacons, logging agents |
| 6 | **Data Seeder** | `SEEDING_PLAN.md`<br>`/seed_data/` | Fake emails, docs, logs, configs, password hints, mobile data |
| 7 | **Compiler/Orchestrator** | Final manifest | Assembles all components into cohesive range |
| 8 | **Scoring Engine** | `SCORING_LOGIC.md`<br>Scoring container | KOTH mechanics, control beacons, lockout system, scoreboard |
| 9 | **Logging Subsystem** | Logging stack | ELK/OpenSearch/Loki setup, event capture, debrief data |
| 10 | **Safety Validator** | `SAFETY_VALIDATION_REPORT.md` | Ensures ethical compliance, fictional content, no unsafe code |
| 11 | **Deployment Coordinator** | `deployment_instructions.md`<br>Running range | Sanity checks, dependency verification, deployment |
| 12 | **Admin Dashboard** | Dashboard UI | Host control, score monitoring, health checks (optional) |
| 13 | **Cleanup Agent** | Clean state | Environment teardown (optional) |

---

## üìÅ Standard Directory Structure

```
/
‚îú‚îÄ‚îÄ README.md                          # Main specification
‚îú‚îÄ‚îÄ CLAUDE.md                          # This file - AI assistant guide
‚îÇ
‚îú‚îÄ‚îÄ infrastructure/                    # ‚úÖ STATIC - Reusable tooling (implement once)
‚îÇ   ‚îú‚îÄ‚îÄ control_beacon/               # Generic beacon software
‚îÇ   ‚îú‚îÄ‚îÄ scoring_engine/               # KOTH scoring logic & scoreboard
‚îÇ   ‚îú‚îÄ‚îÄ org_viewer/                   # Org chart visualization tool
‚îÇ   ‚îú‚îÄ‚îÄ admin_dashboard/              # Admin control panel
‚îÇ   ‚îú‚îÄ‚îÄ logging/                      # Logging stack configs
‚îÇ   ‚îú‚îÄ‚îÄ base_images/                  # Base Docker images
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ workstation/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ server/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mobile/
‚îÇ   ‚îú‚îÄ‚îÄ deployment/                   # Deployment scripts
‚îÇ   ‚îî‚îÄ‚îÄ tools/                        # Seeding/validation utilities
‚îÇ
‚îú‚îÄ‚îÄ agents/                            # ‚úÖ STATIC - AI agent implementations
‚îÇ   ‚îú‚îÄ‚îÄ 01_user_input/
‚îÇ   ‚îú‚îÄ‚îÄ 02_organization_builder/
‚îÇ   ‚îú‚îÄ‚îÄ 03_network_architect/
‚îÇ   ‚îú‚îÄ‚îÄ 04_vulnerability_designer/
‚îÇ   ‚îú‚îÄ‚îÄ 05_image_builder/
‚îÇ   ‚îú‚îÄ‚îÄ 06_data_seeder/
‚îÇ   ‚îú‚îÄ‚îÄ 07_compiler_orchestrator/
‚îÇ   ‚îú‚îÄ‚îÄ 08_scoring_engine_config/
‚îÇ   ‚îú‚îÄ‚îÄ 09_logging_subsystem/
‚îÇ   ‚îú‚îÄ‚îÄ 10_safety_validator/
‚îÇ   ‚îú‚îÄ‚îÄ 11_deployment_coordinator/
‚îÇ   ‚îî‚îÄ‚îÄ 12_admin_dashboard/
‚îÇ
‚îî‚îÄ‚îÄ scenarios/                         # üé≤ DYNAMIC - Generated per CTF
    ‚îú‚îÄ‚îÄ scenario_20240115_143022/     # Unique scenario instance
    ‚îÇ   ‚îú‚îÄ‚îÄ ORGANIZATION.md
    ‚îÇ   ‚îú‚îÄ‚îÄ EMPLOYEE_CREDENTIALS.json
    ‚îÇ   ‚îú‚îÄ‚îÄ NETWORK_TOPOLOGY.md
    ‚îÇ   ‚îú‚îÄ‚îÄ CHALLENGE_DESIGN.md
    ‚îÇ   ‚îú‚îÄ‚îÄ SEEDING_PLAN.md
    ‚îÇ   ‚îú‚îÄ‚îÄ SCORING_LOGIC.md
    ‚îÇ   ‚îú‚îÄ‚îÄ SAFETY_VALIDATION_REPORT.md
    ‚îÇ   ‚îú‚îÄ‚îÄ seed_data/                # Generated content
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ emails/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ documents/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logs/
    ‚îÇ   ‚îú‚îÄ‚îÄ build/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml    # Instance-specific
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ service_configs/
    ‚îÇ   ‚îî‚îÄ‚îÄ runtime/
    ‚îÇ       ‚îú‚îÄ‚îÄ logs/
    ‚îÇ       ‚îî‚îÄ‚îÄ deployment_instructions.md
    ‚îî‚îÄ‚îÄ scenario_20240122_091544/     # Another CTF instance
        ‚îî‚îÄ‚îÄ ...
```

---

## üîë Key Terminology

| Term | Definition |
|------|------------|
| **Host/Device** | Any containerized workstation, service, or simulated phone |
| **Container** | Docker container representing part of the organization |
| **Control Token** | Token proving team control over a host (for KOTH scoring) |
| **Control Beacon** | Small agent inside each host handling token state and scoring |
| **Mobile Device** | Pseudo-phone container for fictional MFA flows (not full emulator) |
| **Logging Subsystem** | Centralized ELK/OpenSearch stack for monitoring all actions |
| **KOTH** | King-of-the-Hill - scoring model where teams capture and hold hosts |
| **HVT** | High-Value Target - critical hosts requiring mobile MFA access |
| **Bounty System** | Point values assigned to employees/hosts (totaling 15,000) |

---

## üíª Development Workflow

### When Implementing Pipeline Components

**FIRST: Determine if you're implementing STATIC or DYNAMIC components**

**For STATIC Infrastructure (implement once, reuse forever):**
1. Place code in `infrastructure/` or `agents/` directories
2. Write generic, parameterized code that accepts configuration
3. Design APIs/interfaces that agents will use
4. Create comprehensive documentation and examples
5. Write unit tests for reusable components
6. Think: "Will this be the same for every CTF scenario?"

**For DYNAMIC Scenario Generation (AI agents creating unique content):**
1. **Read the README.md specification thoroughly** before implementing any agent
2. **Understand the pipeline stage dependencies** - each agent consumes outputs from previous agents
3. **Follow the output artifact naming conventions** exactly as specified
4. **Validate against safety requirements** before proceeding
5. **Use lightweight, open-source components** only
6. **Target resource constraints** (25-30 container max)
7. Output artifacts to `scenarios/{timestamp}/` directories
8. Think: "Will this be different for each CTF scenario?"

### When Adding Code

**File Organization:**
- Place agent implementations in appropriately named directories
- Keep agent code modular and well-documented
- Include configuration examples for each agent
- Provide clear input/output schemas

**Naming Conventions:**
- Agent modules: `{stage_number}_{agent_name}/` (e.g., `02_organization_builder/`)
- Output artifacts: Use EXACT names from specification (e.g., `ORGANIZATION.md`, not `organization.md`)
- Docker images: `cyber-range-{purpose}` (e.g., `cyber-range-workstation`)

**Documentation:**
- Each agent should have its own README explaining inputs, outputs, and usage
- Include example runs and sample outputs
- Document any external dependencies clearly

### Git Workflow

**Current Branch:** `claude/claude-md-miasbyni225didlu-016eohrs6QgjrUSGq9CfuE1p`

**Branching Strategy:**
- Development happens on `claude/*` branches
- Never push directly to main without explicit permission
- Use clear, descriptive commit messages referencing agent/stage

**Commit Message Format:**
```
[Stage N] Brief description

- Detailed change 1
- Detailed change 2
```

Example:
```
[Stage 2] Implement Organization Builder agent

- Add employee credential generator
- Implement bounty point distribution
- Create ORGANIZATION.md template
```

---

## üé® Code Style & Conventions

### Python (Expected Primary Language)

```python
# Use type hints
def generate_credentials(employee: dict) -> dict:
    """Generate fictional credentials for an employee.

    Args:
        employee: Employee profile dict with name, role, bio

    Returns:
        Dict containing work/personal usernames, emails, passwords
    """
    pass

# Follow PEP 8
# Use descriptive variable names
# Include docstrings for all functions
# Avoid hardcoded values - use config files
```

### Docker & Configuration

- Use multi-stage builds for efficiency
- Pin base image versions for reproducibility
- Use environment variables for configuration
- Include health checks in Dockerfile
- Minimize layer count and image size

### JSON/YAML Schemas

```json
{
  "employee_credentials": [
    {
      "employee_id": "string",
      "full_name": "string",
      "work_username": "string",
      "work_email": "string",
      "work_password": "string",
      "personal_username": "string",
      "personal_email": "string",
      "personal_password": "string",
      "password_pattern": "string",
      "credential_reuse": "boolean",
      "mfa_enabled": "boolean",
      "bounty_value": "integer"
    }
  ]
}
```

---

## üõ°Ô∏è Safety & Ethics Guidelines

### Fictional Content Requirements

**Employee Data:**
- Use obviously fictional names (consider using name generators)
- No biographical details matching real individuals
- Avoid patterns that could correlate to real people
- Include diversity in names and roles

**Organizations:**
- Clearly fictional company names
- No mimicking of real company structures
- Thematic consistency (e.g., "Nebula Tech Corp" for tech theme)

**Credentials:**
- Patterns should be educational, not realistic attack training
- Example: `Password123!`, `Summer2024`, `CompanyName!`
- Avoid teaching actual credential stuffing techniques

### Vulnerability Design Constraints

**ALLOWED (High-Level Concepts):**
- ‚úÖ Weak password policies
- ‚úÖ Credential reuse across accounts
- ‚úÖ Information disclosure in logs/documents
- ‚úÖ Misconfigured access controls
- ‚úÖ Unpatched fictional services
- ‚úÖ SQL injection on internal apps (educational)
- ‚úÖ XSS on internal apps (educational)

**NOT ALLOWED (Real-World Exploits):**
- ‚ùå Actual CVE exploitation code
- ‚ùå Zero-day techniques
- ‚ùå Advanced persistence mechanisms
- ‚ùå Kernel exploits
- ‚ùå Hardware attacks
- ‚ùå Social engineering beyond password guessing
- ‚ùå Real MFA bypass techniques

### Logging Ethics

**Log Everything for Learning:**
- All authentication attempts (success/failure)
- Service access patterns
- File access events
- Network connections
- Score submissions
- Command execution (high-level only)

**Privacy Considerations:**
- No logging of real personal data
- Clear data retention policies
- Post-event log review for educational debrief
- Aggregate metrics only for public scoreboard

---

## üèÜ Scoring System Implementation

### King-of-the-Hill Mechanics

**Control Beacon (per host):**
```python
# Pseudocode for control beacon
class ControlBeacon:
    def __init__(self, host_id, control_token, point_value):
        self.host_id = host_id
        self.control_token = control_token
        self.point_value = point_value
        self.controlling_team = None
        self.lockout_until = None

    def submit_token(self, team_id, submitted_token):
        if submitted_token == self.control_token:
            if not self.is_locked_out():
                self.controlling_team = team_id
                self.lockout_until = time.now() + 30  # 30 second lockout
                return True
        return False
```

**Scoring Rules:**
- Each host has a bounty value (points per minute while controlled)
- Total bounty across all hosts = 15,000 points
- Higher-value targets require more steps to reach (pivoting)
- Mobile devices count as hosts with their own bounty values
- HVTs (High-Value Targets) require MFA access

**Lockout Mechanism:**
- After capture: 30-second lockout (configurable)
- Other teams can reclaim after lockout expires
- Encourages both offensive and defensive play

---

## üìä Logging Stack Options

### Option 1: ELK Stack (Elasticsearch + Logstash + Kibana)
- **Pros:** Full-featured, widely known
- **Cons:** Resource-intensive
- **Use when:** Host system has ample resources

### Option 2: OpenSearch + FluentBit + Grafana
- **Pros:** Lighter than ELK, open-source
- **Cons:** Less familiar to some students
- **Use when:** Balancing features and resources

### Option 3: Loki + Promtail + Grafana
- **Pros:** Lightest option, efficient storage
- **Cons:** Fewer search capabilities
- **Use when:** Minimizing resource usage is critical

### Standard Log Format

```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "event_type": "authentication",
  "host_id": "workstation-001",
  "source_ip": "172.20.0.15",
  "username": "jsmith",
  "success": false,
  "details": {
    "service": "ssh",
    "port": 22
  }
}
```

---

## üîç Testing & Validation

### Pre-Deployment Validation Checklist

- [ ] All containers build successfully
- [ ] All networks configured correctly
- [ ] Control beacons operational on all hosts
- [ ] Scoring engine accessible
- [ ] Logging pipeline capturing events
- [ ] All credentials seeded correctly
- [ ] Attack paths verified (at least one path to each HVT)
- [ ] Mobile device containers operational (if used)
- [ ] Scoreboard displays correctly
- [ ] Admin dashboard functional (if implemented)
- [ ] Safety validation passed (no unsafe content)
- [ ] Resource usage within constraints
- [ ] All services offline-only (no external network)

### Agent Output Validation

Each agent should validate its outputs against schemas:
- JSON outputs: Use JSON Schema validation
- Markdown outputs: Ensure all required sections present
- Docker configs: Use `docker-compose config` to validate
- Network configs: Verify CIDR ranges don't conflict

---

## üöÄ Deployment Considerations

### Resource Requirements

**Minimum Host System:**
- 16 GB RAM (32 GB recommended)
- 4 CPU cores (8 recommended)
- 50 GB disk space
- Docker + Docker Compose installed
- No internet required (all images pre-pulled or built locally)

**Container Resource Limits:**
- Workstations: 512 MB - 1 GB RAM
- Servers: 1 GB - 2 GB RAM
- Logging stack: 4 GB - 8 GB RAM
- Scoring engine: 512 MB RAM
- Mobile devices: 256 MB - 512 MB RAM

### Deployment Process

1. **Build Phase:** `docker-compose build` (may take 30-60 min)
2. **Pre-flight Checks:** Run validation script
3. **Launch:** `docker-compose up -d`
4. **Verify:** Check logs, scoreboard, admin dashboard
5. **Seeding:** Run data seeding scripts (if not baked into images)
6. **Ready:** Provide access instructions to teams

---

## ü§ñ AI Assistant Best Practices

### When Working on This Repository

1. **Always read README.md first** - it's the source of truth
2. **Understand STATIC vs DYNAMIC** - distinguish reusable infrastructure from scenario content
3. **Maintain specification purity** - don't modify README.md without explicit user request
4. **Follow the 13-stage pipeline** - understand dependencies before implementing
5. **Prioritize safety** - validate all generated content against safety guidelines
6. **Think about resource constraints** - every container counts against the ~30 limit
7. **Document extensively** - future agents will read your documentation
8. **Use modular design** - each agent should be independently testable
9. **Provide examples** - include sample outputs for each agent
10. **Version artifacts** - use semantic versioning for agent outputs
11. **Think like an educator** - the end goal is student learning

**Critical Decision Tree:**
```
Is this component the same for every CTF scenario?
‚îú‚îÄ YES ‚Üí Implement in infrastructure/ or agents/ (STATIC)
‚îÇ         - Write generic, parameterized code
‚îÇ         - Design for reusability
‚îÇ         - Include in version control
‚îÇ
‚îî‚îÄ NO  ‚Üí Generate per scenario in scenarios/ (DYNAMIC)
          - AI agent produces unique content
          - Instance-specific configuration
          - Timestamped scenario directory
```

### Communication Style

- Be precise about which pipeline stage you're working on
- Reference specific sections of README.md when relevant
- Explain safety/ethical implications of design decisions
- Provide rationale for technical choices
- Flag any deviations from specification for user approval

### Common Pitfalls to Avoid

- ‚ùå Creating realistic-looking credentials that could be mistaken for real data
- ‚ùå Adding complexity beyond the 25-30 container limit
- ‚ùå Implementing actual exploit code instead of high-level concepts
- ‚ùå Skipping the safety validation stage
- ‚ùå Hardcoding values that should be configurable
- ‚ùå Creating heavy containers that exceed resource constraints
- ‚ùå Forgetting to implement logging for certain event types
- ‚ùå Not considering the post-event debrief use case

---

## üìö Key References

### Within Repository
- `README.md` - Complete specification (primary reference)
- `scenario/*.md` - Agent output artifacts (once created)
- `build/docker-compose.yml` - Infrastructure definition (once created)

### External Documentation
- Docker & Docker Compose: Container orchestration
- ELK/OpenSearch/Loki: Logging stack documentation
- CTF design best practices: Educational security competitions
- OWASP Top 10: High-level vulnerability concepts (educational context only)

### Fictional Data Generators
- Faker libraries (Python: `faker`, JS: `faker-js`)
- Random name generators
- Lorem ipsum variants for document content
- Procedural generation for org charts

---

## üîÑ Iteration & Improvement

### Versioning Strategy

**Scenario Versions:**
- Each generated scenario gets a unique ID and timestamp
- Scenarios are reproducible from the same seed inputs
- Version artifacts in `scenario/v{N}/` directories

**Agent Versions:**
- Agents follow semantic versioning (MAJOR.MINOR.PATCH)
- Breaking changes to output format = MAJOR bump
- New features/capabilities = MINOR bump
- Bug fixes = PATCH bump

### Feedback Loop

After each CTF event:
1. Collect student feedback
2. Review logs for unexpected patterns
3. Adjust agent parameters/prompts
4. Update difficulty calibration
5. Refine credential patterns
6. Improve mobile device challenges
7. Optimize resource usage

---

## üéì Educational Goals

Remember: This system exists to teach cybersecurity concepts safely.

### Learning Objectives
- Reconnaissance and enumeration
- Credential-based attacks (high-level)
- Lateral movement concepts
- Privilege escalation patterns
- Log analysis and forensics
- King-of-the-Hill competitive dynamics
- Team coordination and defense

### Post-Event Debrief
- Review logs with students
- Explain attack paths taken
- Discuss defensive measures
- Highlight clever solutions
- Analyze KOTH dynamics
- Review scoring progression

---

## üìû Support & Contribution

### For AI Assistants
- Consult this CLAUDE.md before starting any task
- Reference specific sections when asking for clarification
- Propose changes to conventions if you identify improvements
- Document any new patterns or best practices discovered

### For Human Developers
- This file is maintained for AI assistant guidance
- Treat README.md as immutable specification
- Changes to architecture require updating both README.md and CLAUDE.md
- Keep agent implementations aligned with documented conventions

---

## üèÅ Quick Start for AI Assistants

**If asked to implement STATIC infrastructure (e.g., "build the scoring engine"):**
1. Determine what needs to be configurable vs. hardcoded
2. Create code in `infrastructure/{component_name}/`
3. Design APIs/interfaces that accept scenario-specific parameters
4. Write documentation explaining how agents will use this component
5. Create example configurations showing parameter usage
6. Think: "How will this work with ANY scenario?"
7. Examples: Control beacon, scoreboard UI, org viewer

**If asked to implement an AI AGENT (e.g., "build the organization builder agent"):**
1. Read this CLAUDE.md (you're doing it now!)
2. Read README.md sections for that agent
3. Create agent code in `agents/{stage_number}_{agent_name}/`
4. Review input artifacts from previous agents (if applicable)
5. Define clear input/output schemas
6. Implement the agent to GENERATE scenario-specific content
7. Validate against safety guidelines
8. Test with sample inputs producing different outputs
9. Document usage and provide examples
10. Think: "This agent should produce DIFFERENT outputs each time"

**If asked to "generate a scenario" or "create a CTF":**
1. Run agents sequentially (stages 2-11)
2. Each agent produces artifacts in `scenarios/{timestamp}/`
3. Start with Organization Builder (Stage 2) - it has no dependencies
4. Progress sequentially through stages
5. Validate each stage before proceeding
6. Final output: complete scenario ready for deployment

**If asked to modify existing code:**
1. Read existing implementation thoroughly
2. Determine if it's STATIC or DYNAMIC component
3. Understand impact on downstream agents (if dynamic)
4. Maintain backward compatibility with artifacts
5. Update documentation to reflect changes
6. Validate changes don't break pipeline flow

---

## üìù Document History

**Last Updated:** 2025-11-22
**Repository State:** Specification phase - implementation pending
**Primary Author:** AI-assisted development
**Maintained For:** Claude and other AI assistants working on this codebase

**Recent Updates:**
- 2025-11-22: Added comprehensive static vs. dynamic architecture distinction
- 2025-11-22: Updated directory structure to separate infrastructure/agents/scenarios
- 2025-11-22: Added agent responsibility clarification table
- 2025-11-22: Enhanced development workflow with static/dynamic guidance
- 2025-11-22: Initial CLAUDE.md creation
